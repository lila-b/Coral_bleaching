{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612ed9c2",
   "metadata": {},
   "source": [
    "# Coral Bleaching\n",
    "## Preprocessing\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b165ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea11c656",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34251 entries, 0 to 34250\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Latitude_Degrees            34251 non-null  float64\n",
      " 1   Longitude_Degrees           34251 non-null  float64\n",
      " 2   Ocean_Name                  34251 non-null  object \n",
      " 3   Realm_Name                  34251 non-null  object \n",
      " 4   Country_Name                34251 non-null  object \n",
      " 5   State_Island_Province_Name  34187 non-null  object \n",
      " 6   Distance_to_Shore           34249 non-null  float64\n",
      " 7   Turbidity                   34251 non-null  float64\n",
      " 8   Cyclone_Frequency           34251 non-null  float64\n",
      " 9   Date_Year                   34251 non-null  int64  \n",
      " 10  Depth_m                     32643 non-null  float64\n",
      " 11  Percent_Bleaching           34251 non-null  float64\n",
      " 12  Windspeed                   34251 non-null  float64\n",
      " 13  SSTA                        34251 non-null  float64\n",
      " 14  SSTA_Frequency              34251 non-null  float64\n",
      " 15  SSTA_DHW                    34251 non-null  float64\n",
      " 16  TSA                         34251 non-null  float64\n",
      " 17  TSA_Frequency               34251 non-null  float64\n",
      " 18  TSA_DHW                     34251 non-null  float64\n",
      " 19  Date                        34251 non-null  object \n",
      " 20  Temperature_C               34251 non-null  float64\n",
      " 21  Temperature_Maximum_C       34251 non-null  float64\n",
      " 22  Bleaching_indicator         34251 non-null  int64  \n",
      " 23  Exposure_cat                34251 non-null  float64\n",
      "dtypes: float64(17), int64(2), object(5)\n",
      "memory usage: 6.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "df = pd.read_csv('../data/interim/coral_bleaching_cleaned_v2.csv')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b458b",
   "metadata": {},
   "source": [
    "### Drop Unnecessary Columns and NaNs\n",
    "The current data still has a few categorical features and some missing values that need to be handled. Since there are only two values missing from Distance_to_Shore, I will drop these missing values. However, I will impute missing Depth_m values based on medians by Realm_Name. \n",
    "\n",
    "I have decided to use Realm_Name as the regional feature, which will ultimately be converted using one-hot encoding. The remaining spatial features - Latitude_Degrees, Longitude_Degrees, Ocean_Name, Country_Name, State_Island_Province_Name - and be remove. Additionally, Date and Date_Year can be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc971ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 34249 entries, 0 to 34250\n",
      "Data columns (total 17 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Realm_Name             34249 non-null  object \n",
      " 1   Distance_to_Shore      34249 non-null  float64\n",
      " 2   Turbidity              34249 non-null  float64\n",
      " 3   Cyclone_Frequency      34249 non-null  float64\n",
      " 4   Depth_m                32641 non-null  float64\n",
      " 5   Percent_Bleaching      34249 non-null  float64\n",
      " 6   Windspeed              34249 non-null  float64\n",
      " 7   SSTA                   34249 non-null  float64\n",
      " 8   SSTA_Frequency         34249 non-null  float64\n",
      " 9   SSTA_DHW               34249 non-null  float64\n",
      " 10  TSA                    34249 non-null  float64\n",
      " 11  TSA_Frequency          34249 non-null  float64\n",
      " 12  TSA_DHW                34249 non-null  float64\n",
      " 13  Temperature_C          34249 non-null  float64\n",
      " 14  Temperature_Maximum_C  34249 non-null  float64\n",
      " 15  Bleaching_indicator    34249 non-null  int64  \n",
      " 16  Exposure_cat           34249 non-null  float64\n",
      "dtypes: float64(15), int64(1), object(1)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Keep Realm_Name for imputing and one-hot encoding\n",
    "# drop remaining object features\n",
    "\n",
    "df_2 = df.drop(columns=['Latitude_Degrees', 'Longitude_Degrees','Ocean_Name', 'Country_Name', \n",
    "                          'Date', 'State_Island_Province_Name', 'Date_Year'], axis=1)\n",
    "\n",
    "# drop observations missing distance to shore\n",
    "df_2 = df_2[df_2['Distance_to_Shore'].notna()] \n",
    "df_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b4bee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.707875\n",
       "1    0.292125\n",
       "Name: Bleaching_indicator, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2['Bleaching_indicator'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397daac",
   "metadata": {},
   "source": [
    "### Undersampling & Train/Test Split\n",
    "\n",
    "Due to the imbalance in Bleaching_indicator (70/30), I will use undersampling to work with a more balanced target variable. The data can then be split into training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17185b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16008, 16) (16008,) (4002, 16) (4002,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0    0.501687\n",
       " 1    0.498313\n",
       " Name: Bleaching_indicator, dtype: float64,\n",
       " 1    0.506747\n",
       " 0    0.493253\n",
       " Name: Bleaching_indicator, dtype: float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_2.drop(columns=['Bleaching_indicator'])\n",
    "y = df_2['Bleaching_indicator']\n",
    "\n",
    "# Undersample data to counteract imbalance\n",
    "# i.e. select fewer Bleaching_indicator == 0 observations\n",
    "rus = RandomUnderSampler(random_state=5)\n",
    "X_resample, y_resample = rus.fit_resample(X, y)\n",
    "\n",
    "# split into train and test 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.2, random_state=5)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "y_train.value_counts(normalize=True), y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ec92c",
   "metadata": {},
   "source": [
    "### Impute Missing Depth Values\n",
    "\n",
    "As noted previously, missing Depth_m values will be imputed using the medians grouped by Realm. The code below establishes the medians from the training data, then fills missing values in both training and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e313ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Realm_Name\n",
       "Central Indo-Pacific            6.000\n",
       "Eastern Indo-Pacific            1.800\n",
       "Temperate Australasia           6.000\n",
       "Temperate Northern Atlantic    21.675\n",
       "Temperate Northern Pacific      5.000\n",
       "Tropical Atlantic               8.300\n",
       "Tropical Eastern Pacific        8.900\n",
       "Western Indo-Pacific            5.500\n",
       "Name: Depth_m, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realm_depth_median = X_train.groupby(by=['Realm_Name']).median()['Depth_m']\n",
    "realm_depth_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0249ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_depth(X, medians):\n",
    "\n",
    "    for i in range(len(X)):\n",
    "\n",
    "        if X['Depth_m'].isnull().iloc[i]:\n",
    "            realm = X['Realm_Name'].iloc[i]\n",
    "            median = medians[realm]\n",
    "            X.iloc[i, 4] = median\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ea32a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = fill_missing_depth(X_train, realm_depth_median)\n",
    "X_test = fill_missing_depth(X_test, realm_depth_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e622db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing train data depths: 0\n",
      "Missing test data depths: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Missing train data depths: {X_train.isnull()['Depth_m'].sum()}\")\n",
    "print(f\"Missing test data depths: {X_train.isnull()['Depth_m'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98476ef5",
   "metadata": {},
   "source": [
    "### Scaling (if necessary) \n",
    "\n",
    "While I am mostly considering using various forms of decision trees that do not require scaling, I will scale the numeric data in case it could be useful for a regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1cd463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric features\n",
    "scale_columns = ['Distance_to_Shore', 'Turbidity', 'Cyclone_Frequency', 'Depth_m',\n",
    "                 'Percent_Bleaching', 'Windspeed', 'SSTA', 'SSTA_Frequency', 'SSTA_DHW',\n",
    "                 'TSA', 'TSA_Frequency', 'TSA_DHW', 'Temperature_C',\n",
    "                 'Temperature_Maximum_C', 'Exposure_cat']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[scale_columns])\n",
    "X_test_scaled = scaler.transform(X_test[scale_columns])\n",
    "\n",
    "# convert to df, reinstating original index and column names\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns=scale_columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test.index, columns=scale_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256179ca",
   "metadata": {},
   "source": [
    "### One-Hot Encoding Realm_Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e700c6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Realm_Name back to scaled data\n",
    "X_train_scaled['Realm_Name'] = X_train['Realm_Name']\n",
    "X_test_scaled['Realm_Name'] = X_test['Realm_Name']\n",
    "\n",
    "# Create dummies/one-hot encode for Realm_Name in all X data\n",
    "X_train = pd.get_dummies(X_train, columns=['Realm_Name'], prefix='Realm')\n",
    "X_test = pd.get_dummies(X_test, columns=['Realm_Name'], prefix='Realm')\n",
    "\n",
    "X_train_scaled = pd.get_dummies(X_train_scaled, columns=['Realm_Name'], prefix='Realm')\n",
    "X_test_scaled = pd.get_dummies(X_test_scaled, columns=['Realm_Name'], prefix='Realm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "217feebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train and test data\n",
    "datapath = '../data/processed/'\n",
    "\n",
    "X_train.to_csv(str(datapath)+'X_train_data.csv')\n",
    "X_train_scaled.to_csv(str(datapath)+'X_train_scaled_data.csv')\n",
    "y_train.to_csv(str(datapath)+'y_train_data.csv')\n",
    "\n",
    "X_test.to_csv(str(datapath)+'X_test_data.csv')\n",
    "X_test_scaled.to_csv(str(datapath)+'X_test_scaled_data.csv')\n",
    "y_test.to_csv(str(datapath)+'y_test_data.csv')\n",
    "\n",
    "\n",
    "# index is maintained, remember to reassign in pd.read_csv \n",
    "# pd.read_csv([file], index_col = 'Unnamed: 0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
